- date: 10 Feb 2023
  speaker: Matthias Sachs (Birmingham)
  url: https://www.birmingham.ac.uk/staff/profiles/maths/sachs-matthias.aspx
  zoom: 1
  title: "Learning of tensor-valued quantities with the atomic cluster expansion - from (hyper-)active learning of interatomic forcefields to tensor-valued atomic cluster expansion for learning of dynamical systems"
  abstract: TBC

  
- date: 17 Feb 2023
  speaker: Changpeng Shao (Bristol)
  url: https://research-information.bris.ac.uk/en/persons/changpeng-shao
  zoom: 1
  title: Quantum algorithms for linear regression
  abstract: Quantum computers could solve linear algebra problems much faster than classical computers. In this talk, I will talk about some recent quantum algorithms for linear regression with the goal of outputting a vector solution. First, I will introduce a quantum algorithm that accelerates the technique of leverage score sampling (which is a useful technique in randomised numerical linear algebra). Then I will show how to use it to propose efficient quantum algorithms for solving linear regressions. The talk is mainly based on arXiv 2301.06107.


- date: 24 Feb 2023
  speaker: Rihuan Ke (Bristol)
  url: https://research-information.bris.ac.uk/en/persons/rihuan-ke
  zoom: 1
  title: Hybrid learning for label-efficient solvers for inverse problems and beyond
  abstract: Recent deep learning techniques enable the learning of high-quality inverse problem solvers from data. These techniques handle the ill-posedness of the problems with data, by looking at the examples, and do not require explicit regularisation modelling. Typically, a large set of paired measurements and ground truth solutions is used in order to learn a well-performing model. The learning task can be challenging without such a high-quality dataset, and it is an ill-posed problem itself. In this talk, I will introduce some hybrid learning methods for solving inverse problems in a label-efficient manner. 
  
- date: 3 Mar 2023
  speaker: CANCELLED
  url: TBC
  zoom: 1
  title: TBC
  abstract: TBC
  
- date: 10 Mar 2023
  speaker: Francisco de Lima Andrade (ENS Paris)
  url: TBC
  zoom: 1
  title: Distributed Banach-Picard Iteration for Distributed Inference - Theory and Applications
  abstract: Many inference problems can be mathematically formulated as finding a fixed point of a contractive operator/map. In modern distributed scenarios (e.g., distributed machine learning or sensor networks), this map can be naturally written as the average of individual maps held locally and privately (i.e., the agents don't want to share their local data with the others) by a set of agents linked by a (maybe sparse) communication network. Starting with the classical Banach-Picard iteration (BPI), which is is a widely used natural choice to find fixed points of locally contractive maps, this talk shows how to extend the BPI to these distributed settings. We do not assume that the locally contractive map comes from an underlying optimization problem, which precludes exploiting strong global properties such as convexity, coercivity, or Lipschitzianity. Yet, we present a distributed algorithm (called distributed Banach-Picard iteration DBPI) that keeps the linear convergence rate of the standard BPI for the average locally contractive map. As an application, we derive and prove linear convergence of two distributed algorithms for two classical data analysis problems - the expectation-maximization algorithm for parameter estimation from noisy and faulty distributed sensors; principal component analysis with distributed data (equivalently finding the top m eigenvectors of a positive semidefinite matrix, which is the average of local matrices held by the network agents).   

- date: 17 Mar 2023
  speaker: Eike Mueller (Bath)
  url: https://people.bath.ac.uk/em459/
  zoom: 1
  title: Equivariant and invariant neural networks
  abstract: Equivariance and invariance play a very important role in physics since they strongly constrain the dynamics of a system. For example, Einstein’s famous theory of Special Relativity and Maxwell’s electrodynamics follow naturally if we assume that the equations of motion are equivariant under Lorentz transformations. Recently, there has been a lot of work on constructing equivariant neural networks. A well-known example are Convolutional Neural Networks (CNNs) for image classification, in which the output is invariant under translations - the network doesn’t care whether a cat appears in the upper left or lower right corner of an image. Here translational invariance provides a strong prior, which allows the network to generalise from limited training data. Equivariance under other symmetry groups such as rotations can be incorporated into the architecture of neural networks in similar ways. In this session I will try to summarise some of the main ideas, based on some papers I came across recently. I’m by no means an expert in this area, so this will be a very informal session, hopefully with lively and interesting discussions.
  
- date: 24 Mar 2023
  speaker: Philip J Herbert (Heriot-Watt)
  url: TBC
  zoom: 1
  title: Shape optimisation with Lipschitz functions
  abstract: In this talk, we discuss a novel method in PDE constrained shape optimisation.  We begin by introducing the concept of PDE constrained shape optimisation.  While it is known that many shape optimisation problems have a solution, their approximation in a meaningful way is non-trivial.  To find a minimiser, it is typical to use first order methods.  The novel method we propose is to deform the shape with fields which are a direction of steepest descent in the topology of $W^{1,\infty}$.  We present an analysis of this in a discrete setting along with the existence of directions of steepest descent.  Several numerical experiments will be considered which compare a classical Hilbertian approach to this novel approach.
  
- date: 31 Mar 2023
  speaker: Arieh Iserles (Cambridge)
  url: http://www.damtp.cam.ac.uk/user/ai/Arieh_Iserles/Arieh_Iserles.html
  zoom: 1
  title: An overarching framework for spectral methods and dispersive equations
  abstract: Many invariants of time-evolving PDEs, e.g. mass and some Hamiltonians, can be formulated as a bilinear form. In this talk we are concerned with formal orthogonal systems on the real line (and, by tensorisation, in $\mathbb{R}^d$) with respect to bilinear forms and with a tridiagonal differentiation matrix. The theory is virtually complete for $L_2$ and Sobolev inner products - using a Fourier transform we show that such systems are in a one-to-one relationship with determinate Borel measures and that their closure is a Paley–Wiener space. We provide several examples, commencing from the familiar Hermite functions. We also characterise all such systems that can be computed fast – for $L_2$ orthogonality using Fast Fourier/Cosine/Sine Transform, for Sobolev orthogonality the above in tandem with a narrow-banded matrix of connection coefficients. We conclude with preliminary results on systems that are orthogonal with respect to a bilinear form generated by the Hamiltonian of a linear Schrödinger equation. This is joint work with Marcus Webb.

- date: 21 Apr 2023
  speaker: Kostas Papafitsoros (QMUL)
  url: https://www.qmul.ac.uk/maths/profiles/papafitsorosk.html
  zoom: 1
  title: TBC
  abstract: TBC

- date: 28 Apr 2023
  speaker: Jemima M. Tabeart (Oxford)
  url: https://jemimat.github.io/
  zoom: 1
  title: TBC
  abstract: TBC