- date: 5 Feb 2021
  speaker: Alberto Paganini (Leicester)
  url: https://www2.le.ac.uk/departments/mathematics/extranet/staff-material/staff-profiles/alberto-paganini
  teams: https://teams.microsoft.com/l/meetup-join/19%3ad8aa162230c14a67a10243ac2f75d98d%40thread.tacv2/1612172995109?context=%7b%22Tid%22%3a%22377e3d22-4ea1-422d-b0ad-8fcc89406b9e%22%2c%22Oid%22%3a%22ca31603c-bea1-49a9-9542-de84a57ad77c%22%7d
  title: Automated shape optimization with finite elements
  abstract: Shape optimization studies how to design a domain such that a shape function is minimized. Ubiquitous in industrial applications, shape optimization is often constrained to partial differential equations (PDEs). In such instances, deriving shape derivative formulas by hand and coupling domain updates with PDE-solvers can be challenging, if not discouraging. In this talk, we give a gentle introduction to shape optimization and illustrate how finite element techniques allow automating shape optimization and hence tackling challenging PDE-constrained problems with ease.

- date: 12 Feb 2021
  speaker: Ronald Morgan (Baylor University, US)
  inred: 'different time: 2:15 pm'
  url: https://www.baylor.edu/math/index.php?id=54013
  teams: https://teams.microsoft.com/l/meetup-join/19%3ad8aa162230c14a67a10243ac2f75d98d%40thread.tacv2/1612172995109?context=%7b%22Tid%22%3a%22377e3d22-4ea1-422d-b0ad-8fcc89406b9e%22%2c%22Oid%22%3a%22ca31603c-bea1-49a9-9542-de84a57ad77c%22%7d
  title: Solving Large Systems of Linear Equations or Monster Matrices and How to Care for Them
  abstract: We look at Krylov methods for solving large systems of linear equations. Convergence theory will be given in a hopefully easy way to understand, at least for people who like surfing. Then deflating eigenvalues is mentioned, and deflation is used in a two-grid BiCGStab method. Also, a new stable polynomial preconditioning approach will be given. Next, Krylov methods are developed for rank-one updated systems and maybe there will be time to apply this to singular matrices. Bears will be mentioned along the way including a certain English bear named Winnie. However, no animals will be harmed by this talk except for the feelings of a certain giraffe. 

- date: 19 Feb 2021
  speaker: Leon Bungert (Erlangen, Germany)
  url: https://sites.google.com/view/leon-bungert
  teams: https://teams.microsoft.com/l/meetup-join/19%3ad8aa162230c14a67a10243ac2f75d98d%40thread.tacv2/1612172995109?context=%7b%22Tid%22%3a%22377e3d22-4ea1-422d-b0ad-8fcc89406b9e%22%2c%22Oid%22%3a%22ca31603c-bea1-49a9-9542-de84a57ad77c%22%7d
  title: 	Continuum Limit for Lipschitz Learning on Graphs
  abstract: "In semi-supervised learning one is confronted with a large set of data points, only very few of which are labelled. The task is to find a labelling function which extends these labels to the whole data set. In order to find useful labelling functions, in graph-based semi-supervised learning one represents the data set as weighted graph and poses a smoothness constraint on the labelling function. In this context p-Laplacian learning has become very popular and consists in finding a p-harmonic function which coincides with labels on the labelled set. However, this method is asymptotically ill-posed if p is smaller than the dimension of the data space, and is not feasible for most applications. 
  
  In this work, I will therefore speak about Lipschitz-learning which aims to find a Lipschitz-extension of the labels and is well-posed in arbitrary dimension. The main result is a discrete-to-continuum limit of Lipschitz-extensions as the data set grows to a continuum. Our theory uses Gamma-convergence and Hausdorff convergence of the data set. As a by-product we obtain a continuum limit for a nonlinear eigenvalue problem related to geodesic distance functions."
  
- date: 26 Feb 2021
  speaker: Rob Scheichl (Heidelberg, Germany)
  url: https://ganymed.math.uni-heidelberg.de/~rscheichl/
  teams: https://teams.microsoft.com/l/meetup-join/19%3ad8aa162230c14a67a10243ac2f75d98d%40thread.tacv2/1612172995109?context=%7b%22Tid%22%3a%22377e3d22-4ea1-422d-b0ad-8fcc89406b9e%22%2c%22Oid%22%3a%22ca31603c-bea1-49a9-9542-de84a57ad77c%22%7d
  title: Optimal local approximation spaces for generalized finite element methods
  abstract: In this talk, I present new optimal local approximation spaces for the generalized finite element method for solving second order elliptic equations with rough coefficients, which are based on local eigenvalue problems involving the partition of unity. In addition to a nearly exponential decay rate of the local approximation error with respect to the dimension of the local spaces, I also give the rate of convergence with respect to the size of the oversampling region. To reduce the computational cost of the method, an efficient and easy-to-implement technique for generating the necessary discrete A-harmonic spaces is proposed. Numerical experiments are presented to support the theoretical analysis and confirm the effectiveness of the method. This is joint work with Chupeng Ma (Heidelberg).
  
- date: 5 Mar 2021
  speaker: Dante Kalise (Nottingham)
  url: https://sites.google.com/view/dkalise
  teams: https://teams.microsoft.com/l/meetup-join/19%3ad8aa162230c14a67a10243ac2f75d98d%40thread.tacv2/1612172995109?context=%7b%22Tid%22%3a%22377e3d22-4ea1-422d-b0ad-8fcc89406b9e%22%2c%22Oid%22%3a%22ca31603c-bea1-49a9-9542-de84a57ad77c%22%7d
  title: Synthetic data-driven methods for approximating high-dimensional Hamilton-Jacobi PDEs
  abstract: "Hamilton-Jacobi PDEs are central in control and differential games, enabling the computation of optimal control laws expressed in feedback form. High-dimensional HJ PDEs naturally arise in the feedback synthesis for high-dimensional control systems, and their numerical solution must be sought outside the framework provided by standard grid-based discretizations methods. 
  In this talk, I will discuss the construction of causality-free, data-driven methods for the approximation of high-dimensional HJ PDEs. I will address the generation of a synthetic dataset based on the use of representation formulas (such as Lax-Hopf or Pontryagin's Maximum Principle), which is then fed into a high-dimensional sparse polynomial/ANN model for training. The use of representation formulas providing gradient information is fundamental to increase the data efficiency of the method. I will present applications in nonlinear dynamics, control of PDEs, and agent-based models."

- date: 12 Mar 2021
  speaker: Eike Mueller (Bath)
  url: https://people.bath.ac.uk/em459/
  teams: https://teams.microsoft.com/l/meetup-join/19%3ad8aa162230c14a67a10243ac2f75d98d%40thread.tacv2/1612172995109?context=%7b%22Tid%22%3a%22377e3d22-4ea1-422d-b0ad-8fcc89406b9e%22%2c%22Oid%22%3a%22ca31603c-bea1-49a9-9542-de84a57ad77c%22%7d
  title: Multilevel Monte Carlo for quantum mechanics on a lattice
  abstract: "Monte Carlo simulations of quantum field theories on a lattice become increasingly expensive as the continuum limit is approached since the cost per independent sample grows with a high power of the inverse lattice spacing. Simulations on fine lattices suffer from critical slowdown, the rapid growth of autocorrelations in the Markov chain with decreasing lattice spacing a. This causes a strong increase in the number of lattice configurations that have to be generated to obtain statistically significant results. We discuss hierarchical sampling methods to tame this growth in autocorrelations; combined with multilevel variance reduction techniques, this significantly reduces the computational cost of simulations.
 
We recently demonstrated the efficiency of this approach for two non-trivial model systems in quantum mechanics in https://arxiv.org/abs/2008.03090. This includes a topological oscillator, which is badly affected by critical slowdown due to freezing of the topological charge. On fine lattices our methods are several orders of magnitude faster than standard, single level sampling based on Hybrid Monte Carlo. For very high resolutions, multilevel Monte Carlo can be used to accelerate even the cluster algorithm, which is known to be highly efficient for the topological oscillator. Performance is further improved through perturbative matching. This guarantees efficient coupling of theories on the multilevel lattice hierarchy, which have a natural interpretation in terms of effective theories obtained by renormalisation group transformations.
 
At the end I will also present some very recent results on how the methods can be extended to a two dimensional lattice gauge theory, namely the 2d Schwinger model, a simplified toy model for quantum electrodynamics."
  
- date: 19 Mar 2021
  inred: Cancelled
  
- date: 26 Mar 2021
  speaker: Tiangang Cui (Monash University, Australia)
  url: https://www.fastfins.org/
  teams: https://teams.microsoft.com/l/meetup-join/19%3ad8aa162230c14a67a10243ac2f75d98d%40thread.tacv2/1612172995109?context=%7b%22Tid%22%3a%22377e3d22-4ea1-422d-b0ad-8fcc89406b9e%22%2c%22Oid%22%3a%22ca31603c-bea1-49a9-9542-de84a57ad77c%22%7d
  title: 	Intrinsic subspaces of high-dimensional inverse problems and where to find them
  abstract: The high-dimensionality is a central challenge faced by many numerical methods for solving large-scale Bayesian inverse problems. In this talk, we will present some old and new developments in the identification of low-dimensional subspaces that offer a viable path to alleviating this dimensionality barrier. Utilising concentration inequalities, we are able to identify the intrinsic subspaces from the solutions of certain eigenvalue problems and derive corresponding dimension-truncation error bounds. The resulting low-dimensional subspace enables the design of inference algorithms that can scale sub-linearly with the apparent dimensionality of the problem.
  
- date: 16 Apr 2021
  speaker: James Foster (Oxford)
  url: https://www.maths.ox.ac.uk/people/james.foster
  teams: https://teams.microsoft.com/l/meetup-join/19%3ad8aa162230c14a67a10243ac2f75d98d%40thread.tacv2/1612172995109?context=%7b%22Tid%22%3a%22377e3d22-4ea1-422d-b0ad-8fcc89406b9e%22%2c%22Oid%22%3a%22ca31603c-bea1-49a9-9542-de84a57ad77c%22%7d
  title: 	High order numerical simulation of the underdamped Langevin diffusion
  abstract: The underdamped Langevin diffusion (ULD) is an important model in statistical mechanics and has recently seen applications in data science for sampling from high-dimensional distributions. In this talk, I will present a new approach to the numerical approximation of ULD. Our strategy is to first reduce the underdamped Langevin SDE to a comparable ODE, before then applying an appropriate ODE solver. For strongly convex potentials with Lipschitz continuous derivatives, we show that this ODE approximation is ergodic and obtain non-asymptotic estimates for the 2-Wasserstein error of its stationary distribution. Moreover, by discretizing this ODE using a third order Runge-Kutta method, we obtain a practical method that uses just two additional gradient evaluations per step. When applied to a logistic regression problem, this method empirically shows a third order convergence rate and outperforms other ULD-based sampling algorithms.
  
- date: 23 Apr 2021
  speaker: Sergey Dolgov (Bath)
  url: https://people.bath.ac.uk/sd901/
  teams: https://teams.microsoft.com/l/meetup-join/19%3ad8aa162230c14a67a10243ac2f75d98d%40thread.tacv2/1612172995109?context=%7b%22Tid%22%3a%22377e3d22-4ea1-422d-b0ad-8fcc89406b9e%22%2c%22Oid%22%3a%22ca31603c-bea1-49a9-9542-de84a57ad77c%22%7d
  title: 	Deep tensor decompositions for sampling from high-dimensional distributions
  abstract: 'Characterising intractable high-dimensional random variables is one of the fundamental challenges in stochastic computation, for example, in the solution of Bayesian inverse problems. The recent surge of transport maps offers a mathematical foundation and new insights for tackling this challenge by coupling intractable random variables with tractable reference random variables.

In this talk I will present a nested coordinate transformation framework inspired by deep neural networks but driven by functional tensor-train approximation of tempered probability density functions instead. This bypasses slow gradient descent optimisation by a direct inverse Rosenblatt transformation. The resulting deep inverse Rosenblatt transport significantly expands the capability of tensor approximations and transport maps to random variables with complicated nonlinear interactions and concentrated density functions. We demonstrate the efficiency of the proposed approach on a range of applications in uncertainty quantification, including parameter estimation for dynamical systems and inverse problems constrained by partial differential equations.'
  
- date: 30 Apr 2021
  speaker: Yuya Suzuki (NTNU, Norway)
  url: https://www.ntnu.edu/employees/yuya.suzuki
  teams: https://teams.microsoft.com/l/meetup-join/19%3ad8aa162230c14a67a10243ac2f75d98d%40thread.tacv2/1612172995109?context=%7b%22Tid%22%3a%22377e3d22-4ea1-422d-b0ad-8fcc89406b9e%22%2c%22Oid%22%3a%22ca31603c-bea1-49a9-9542-de84a57ad77c%22%7d
  title: 	'Rank-1 and rank-r lattices combined with operator splitting for time-dependent Schrödinger equations'
  abstract: 'In this talk, we use lattice points combined with operator splitting for numerically solving time-dependent Schrödinger equations. Here "lattice points" refers to a specific point set coming from the context of quasi-Monte Carlo methods. We propose a Fourier pseudospectral method on lattice points combined with operator splitting, and we prove that our method converges with the desired order of splitting method, given that the potential function is in a Korobov space with a certain smoothness which is independent of the dimension of the problem. We conduct numerical experiments in various settings. Comparing with the sparse grid method, our method is shown to be more efficient. Even in higher dimensions and with higher-order splitting schemes, our method shows stable and higher-order convergence numerically. One of the essential tasks here is to compute the Fourier transform and the inverse transform repeatedly in a higher-dimensional space for simulating the time-stepping operator of the time-dependent Schrödinger equation in a stable manner. Our proposed method solves this task efficiently.'

- date: 7 May 2021
  speaker: MAD Workshop
  url: https://mathematics-and-algorithms-for-data.github.io/events/workshop2/
  title:  "Estimating functions from data"